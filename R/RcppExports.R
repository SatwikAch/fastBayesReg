# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#'@importFrom Rcpp evalCpp
#'@importFrom pgdraw pgdraw
#'@import bigmemory
#'@import BH
#'@useDynLib fastBayesReg, .registration=TRUE
NULL

#'@title Accurately compute log(1-exp(-x)) for x > 0
#'@param x a vector of nonnegative numbers
#'@return a vector of values of log(1-exp(-x))
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'x <- seq(0,10,length=1000)
#'y <- log1mexpm(x)
#'plot(x,y,type="l")
#'@export
log1mexpm <- function(x) {
    .Call(`_fastBayesReg_log1mexpm`, x)
}

#'@title Accurately compute log(1+exp(x))
#'@param x a vector of real numbers
#'@return a vector of values of log(1+exp(x))
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'x <- seq(-100,100,length=1000)
#'y <- log1pexp(x)
#'plot(x,y,type="l")
#'@export
log1pexp <- function(x) {
    .Call(`_fastBayesReg_log1pexp`, x)
}

log1pexp_mat <- function(x) {
    .Call(`_fastBayesReg_log1pexp_mat`, x)
}

#'@title Simulate data from the linear regression model
#'@param n sample size
#'@param p number of candidate predictors
#'@param q number of nonzero predictors
#'@param R2 R-squared indicating the proportion of variation explained by the predictors
#'@param beta_size effect size of beta coefficients
#'@param X_cor correlation between covariates
#'@return a list objects consisting of the following components
#'\describe{
#'\item{y}{vector of n outcome variables}
#'\item{X}{n x p matrix of candidate predictors}
#'\item{betacoef}{vector of p regression coeficients}
#'\item{R2}{R-squared indicating the proportion of variation explained by the predictors}
#'\item{sigma2}{noise variance}
#'\item{X_cor}{correlation between covariates}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat<-sim_linear_reg(n=25,p=2000,X_cor=0.9,q=6)
#'summary(with(dat,lm(y~X)))
#'@export
sim_linear_reg <- function(n = 100L, p = 20L, q = 5L, R2 = 0.95, X_cor = 0.5, beta_size = 1) {
    .Call(`_fastBayesReg_sim_linear_reg`, n, p, q, R2, X_cor, beta_size)
}

#'@title Simulate data from the linear regression model with multiple outcome variables
#'@param n sample size
#'@param m number of outcome variables
#'@param p number of candidate predictors
#'@param q number of nonzero predictors
#'@param R2 R-squared indicating the proportion of variation explained by the predictors
#'@param beta_size effect size of beta coefficients
#'@param X_cor correlation between covariates
#'@return a list objects consisting of the following components
#'\describe{
#'\item{y}{vector of n outcome variables}
#'\item{X}{n x p matrix of candidate predictors}
#'\item{betacoef}{vector of p regression coeficients}
#'\item{R2}{R-squared indicating the proportion of variation explained by the predictors}
#'\item{sigma2}{noise variance}
#'\item{X_cor}{correlation between covariates}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat<-sim_linear_reg_multi(n=2500,p=200,m=5,X_cor=0.9,q=6)
#'summary(with(dat,lm(y~X)))
#'@export
sim_linear_reg_multi <- function(n = 100L, p = 20L, m = 5L, q = 5L, R2 = 0.95, X_cor = 0.5, beta_size = 1) {
    .Call(`_fastBayesReg_sim_linear_reg_multi`, n, p, m, q, R2, X_cor, beta_size)
}

#'@title Simulate data from the logistic regression model
#'@param n sample size
#'@param p number of candidate predictors
#'@param q number of nonzero predictors
#'@param R2 R-squared indicating the proportion of variation explained by the predictors
#'@param beta_size effect size of beta coefficients
#'@param X_cor correlation between covariates
#'@param density specifies the percentage of non-zero elements. The default value is 1.0 in which case
#'X is a dense matrix. When the density is strictly less than 1.0 in which case X is a spare matrix.
#'@return a list objects consisting of the following components
#'\describe{
#'\item{y}{vector of n outcome variables}
#'\item{X}{n x p matrix of candidate predictors}
#'\item{betacoef}{vector of p regression coeficients}
#'\item{R2}{R-squared indicating the proportion of variation explained by the predictors}
#'\item{sigma2}{noise variance}
#'\item{X_cor}{correlation between covariates}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat<-sim_logit_reg(n=1000,p=20,X_var = 1,X_cor=0.9,q=10)
#'summary(with(dat,glm(y~0+X,family=binomial())))
#'@export
sim_logit_reg <- function(n = 100L, p = 20L, q = 5L, X_cor = 0.5, X_var = 10, beta_size = 1, density = 1.0) {
    .Call(`_fastBayesReg_sim_logit_reg`, n, p, q, X_cor, X_var, beta_size, density)
}

#'@title Simulate data from the multinomial logistic regression model
#'@param K number of class
#'@param n sample size
#'@param p number of candidate predictors
#'@param q number of nonzero predictors
#'@param R2 R-squared indicating the proportion of variation explained by the predictors
#'@param X_cor correlation between covariates
#'@param X_var marginal variance of covariates
#'@param beta_size effect size of beta coefficients
#'@param intercept_size intercepts
#'@param intercept0 a vector of (K-1) intercepts
#'@return a list objects consisting of the following components
#'\describe{
#'\item{y}{vector of n outcome variables}
#'\item{X}{n x p matrix of candidate predictors}
#'\item{betacoef}{n x (K-1) matrix of regression coeficients}
#'\item{intercept}{a vector of (K-1) intercept coeficients}
#'\item{R2}{R-squared indicating the proportion of variation explained by the predictors}
#'\item{sigma2}{noise variance}
#'\item{X_cor}{correlation between covariates}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,beta_size=1,intercept0=c(5,-5,-10,-10))
#'glmnet_res <- with(dat,wrap_glmnet(y,cbind(1,X),family="multinomial"))
#'Bayes_res <- with(dat,fast_normal_multiclass(y,cbind(1,X),num_class=length(unique(y)),burnin=5000))
#'glmnet_pred <- as.numeric(predict(glmnet_res$glmnet_fit,newx = cbind(1,dat$X),type = "class"))
#'Bayes_pred <- apply(Bayes_res$post_mean$prob,1,which.max)-1
#'print(c(glmnet_acc = mean(glmnet_pred==dat$y),Bayes_acc = mean(Bayes_pred==dat$y)))
#'
#'@export
sim_multiclass_reg <- function(K = 5L, n = 100L, p = 20L, q = 5L, X_cor = 0.5, X_var = 10, beta_size = 1, intercept_size = 1, intercept0 = NULL) {
    .Call(`_fastBayesReg_sim_multiclass_reg`, K, n, p, q, X_cor, X_var, beta_size, intercept_size, intercept0)
}

#'@title Fast Bayesian linear regression with normal priors
#'@param y vector of n outcome variables
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{mu}{a vector of posterior predictive mean of the n training sample}
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{mu}{a vector of posterior predictive mean of the n training sample}
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_normal_lm(y,X))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_normal_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_normal_tab <- tab
#'print(fast_normal_tab)
#'@export
fast_normal_lm <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, a_sigma = 0.01, b_sigma = 0.01, A_tau = 10) {
    .Call(`_fastBayesReg_fast_normal_lm`, y, X, mcmc_sample, burnin, thinning, a_sigma, b_sigma, A_tau)
}

#'@title Fast Bayesian linear regression with normal priors with multiple outcome variables
#'@param y n x q matrix of q outcome variables with n observations
#'@param X n x p matrix of p candidate predictors with n observations
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{mu}{a matrix of posterior predictive mean of the n training sample}
#'\item{betacoef}{a matrix of posterior mean of q x p regression coeficients}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{mu}{an array of posterior predictive mean of the n training sample}
#'\item{betacoef}{an array of posterior mean of p by q regression coeficients}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2024)
#'dat1 <- sim_linear_reg_multi(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_normal_multi_lm(y,X,mcmc_output=FALSE))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_normal_multi_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_normal_tab <- tab
#'print(fast_normal_tab)
#'@export
fast_normal_multi_lm <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, a_sigma = 0.01, b_sigma = 0.01, A_tau = 10, mcmc_output = TRUE) {
    .Call(`_fastBayesReg_fast_normal_multi_lm`, y, X, mcmc_sample, burnin, thinning, a_sigma, b_sigma, A_tau, mcmc_output)
}

#'@title Fast Bayesian logistic regression with normal priors
#'@param y vector of n binrary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res1 <- with(dat1,fast_normal_logit(y,X))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res2 <- with(dat2,fast_normal_logit(y,X,burnin=5000))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
fast_normal_logit <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1) {
    .Call(`_fastBayesReg_fast_normal_logit`, y, X, mcmc_sample, burnin, thinning, A_tau)
}

#'@title Fast Bayesian logistic regression with normal priors by single
#'variable update Gibbs sampler
#'@param y vector of n binrary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res1 <- with(dat1,fast_normal_logit_single_gibbs(y,X))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res2 <- with(dat2,fast_normal_logit_single_gibbs(y,X,burnin=5000))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
fast_normal_logit_single_gibbs <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_fast_normal_logit_single_gibbs`, y, X, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Scalable Bayesian logistic regression with normal priors by single
#'variable update Gibbs sampler
#'@param y vector of n binrary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res1 <- with(dat1,scalable_normal_logit_single_gibbs(y,X))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res2 <- with(dat2,scalable_normal_logit_single_gibbs(y,X,burnin=5000))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
scalable_normal_logit_single_gibbs <- function(y, bigX, rowidx, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_scalable_normal_logit_single_gibbs`, y, bigX, rowidx, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Bayesian logistic regression with normal priors by single
#'variable update Gibbs sampler when predictor matrix is a big.matrix
#'@param y vector of n binary outcome variables taking values 0 or 1
#'@param X n x p sparse matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'library(bigmemory)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'dat1$X <- as.big.matrix(dat1$X)
#'res1 <- with(dat1,big_normal_logit_single_gibbs(y,X@address))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res2 <- with(dat2,big_normal_logit_single_gibbs(y,X@address,burnin=5000))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
big_normal_logit_single_gibbs <- function(y, bigX, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_big_normal_logit_single_gibbs`, y, bigX, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Bayesian logistic regression with normal priors by single
#'variable update Gibbs sampler sparse predictor matrices
#'@param y vector of n binary outcome variables taking values 0 or 1
#'@param X n x p sparse matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5,density=0.1)
#'res1 <- with(dat1,sparse_normal_logit_single_gibbs(y,X))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5,density=0.1)
#'res2 <- with(dat2,sparse_normal_logit_single_gibbs(y,X,burnin=5000))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
sparse_normal_logit_single_gibbs <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_sparse_normal_logit_single_gibbs`, y, X, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Fast Bayesian multinomial logistic regression with normal priors
#'@param y vector of n multiclass outcome variables taking values 0,...,M-1
#'@param X n x p matrix of candidate predictors
#'@param num_class an integer indicating the number of classes
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,beta_size=1,intercept0=c(5,-5,-10,-10))
#'glmnet_res <- with(dat,wrap_glmnet(y,cbind(1,X),family="multinomial"))
#'Bayes_res <- with(dat,fast_normal_multiclass(y,cbind(1,X),num_class=length(unique(y)),burnin=5000))
#'glmnet_pred <- as.numeric(predict(glmnet_res$glmnet_fit,newx = cbind(1,dat$X),type = "class"))
#'Bayes_pred <- apply(Bayes_res$post_mean$prob,1,which.max)-1
#'print(c(glmnet_acc = mean(glmnet_pred==dat$y),Bayes_acc = mean(Bayes_pred==dat$y)))
#'@export
fast_normal_multiclass <- function(y, X, num_class, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1) {
    .Call(`_fastBayesReg_fast_normal_multiclass`, y, X, num_class, mcmc_sample, burnin, thinning, A_tau)
}

#'@title Fast Bayesian multinomial logistic regression with normal priors using single gibbs samplers
#'@param y vector of n multiclass outcome variables taking values 0,...,M-1
#'@param X n x p matrix of candidate predictors
#'@param num_class an integer indicating the number of classes
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,beta_size=1,intercept0=c(5,-5,-10,-10))
#'glmnet_res <- with(dat,wrap_glmnet(y,cbind(1,X),family="multinomial"))
#'Bayes_res <- with(dat,fast_normal_multiclass_single_gibbs(y,cbind(1,X),num_class=length(unique(y)),burnin=5000))
#'glmnet_pred <- as.numeric(predict(glmnet_res$glmnet_fit,newx = cbind(1,dat$X),type = "class"))
#'Bayes_pred <- apply(Bayes_res$post_mean$prob,1,which.max)-1
#'print(c(glmnet_acc = mean(glmnet_pred==dat$y),Bayes_acc = mean(Bayes_pred==dat$y)))
#'@export
fast_normal_multiclass_single_gibbs <- function(y, X, num_class, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_fast_normal_multiclass_single_gibbs`, y, X, num_class, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Memory efficient Bayesian multinomial logistic regression with normal priors using single gibbs samplers
#'@param y vector of n multiclass outcome variables taking values 0,...,M-1
#'@param X n x p matrix of candidate predictors
#'@param num_class an integer indicating the number of classes
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,beta_size=1,intercept0=c(5,-5,-10,-10))
#'glmnet_res <- with(dat,wrap_glmnet(y,cbind(1,X),family="multinomial"))
#'Bayes_res <- with(dat,scalable_normal_multiclass_single_gibbs(y,cbind(1,X),num_class=length(unique(y)),burnin=5000))
#'glmnet_pred <- as.numeric(predict(glmnet_res$glmnet_fit,newx = cbind(1,dat$X),type = "class"))
#'Bayes_pred <- apply(Bayes_res$post_mean$prob,1,which.max)-1
#'print(c(glmnet_acc = mean(glmnet_pred==dat$y),Bayes_acc = mean(Bayes_pred==dat$y)))
#'@export
scalable_normal_multiclass_single_gibbs <- function(y, X, num_class, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, verbose = 0L) {
    .Call(`_fastBayesReg_scalable_normal_multiclass_single_gibbs`, y, X, num_class, mcmc_sample, burnin, thinning, A_tau, verbose)
}

#'@title Fast mean field variational Bayesian logistic regression with normal priors
#'@param y vector of n binrary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param max_iter maximum number of iterations
#'@param tol the tolerance for the parameter changes
#'@param A scale parameter in the half Cauchy prior for regression coefficients
#'@param in_E_inv_tau_sq numeric scalar for the initial value for inverse of tau squared
#'@param in_E_omega numeric vector of length n for the initial values for all omega's
#'@param in_E_beta  numeric vector of length p for the initial values for all beta's
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{inv_tau_sq}{posterior mean of inverse tau square}
#'\item{omega}{a vector of posterior mean of omega}
#'}
#'\item{trace}{a list object of two components for the trace of parameter updates}
#'\describe{
#'\item{inv_tau_sq}{a vector of trace for inverse of tau_sq}
#'\item{sum_beta_sq}{a vector of trace for inverse of summation of beta_sq}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=20,X_cor=0.5,X_var=1,q=10,beta_size=5)
#'split <- train_test_splits(nrow(dat1$X))
#'dat1$train_idx <- split$train_idx
#'dat1$test_idx <- split$test_idx
#'res1 <- with(dat1,fast_mfvb_normal_logit(y[train_idx],X[train_idx,]))
#'res1_mcmc <- with(dat1,fast_normal_logit(y[train_idx],X[train_idx,]))
#'res1_glmnet <- with(dat1,wrap_glmnet(y[train_idx],X[train_idx,],alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=400,X_cor=0.5,X_var=2,q=10,beta_size=1)
#'split <- train_test_splits(nrow(dat2$X))
#'dat2$train_idx <- split$train_idx
#'dat2$test_idx <- split$test_idx
#'res2 <- with(dat2,fast_mfvb_normal_logit(y[train_idx],X[train_idx,]))
#'res2_mcmc <- with(dat2,fast_normal_logit(y[train_idx],X[train_idx,]))
#'res2_glmnet <- with(dat1,wrap_glmnet(y[train_idx],X[train_idx,],alpha=0.5,family=binomial()))
#'tab1 <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_mcmc$post_mean$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,
#'res1_mcmc$elapsed))
#'tab2 <- data.frame(rbind(comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_mcmc$post_mean$betacoef)),
#'time=c(res2$elapsed,res2_glmnet$elapsed,
#'res2_mcmc$elapsed))
#'rownames(tab1)<-c("n = 2000, p = 20 MFVB","n = 2000, p = 20 glmnet",
#'"n = 2000, p = 20 MCMC")
#'rownames(tab2)<-c("n = 200, p = 400 MFVB","n = 200, p = 400 glmnet",
#'"n = 200, p = 400 MCMC")
#'mfvb_logit_tab <- rbind(tab1,tab2)
#'print(mfvb_logit_tab)
#'@export
fast_mfvb_normal_logit <- function(y, X, max_iter = 5000L, tol = 1e-05, A = 10, in_E_inv_tau_sq = 1, in_E_omega = NULL, in_E_beta = NULL) {
    .Call(`_fastBayesReg_fast_mfvb_normal_logit`, y, X, max_iter, tol, A, in_E_inv_tau_sq, in_E_omega, in_E_beta)
}

#'@title Fast single variable update mean field variational Bayesian
#'logistic regression with normal priors
#'@param y vector of n binrary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param max_iter maximum number of iterations
#'@param tol the tolerance for the parameter changes
#'@param A scale parameter in the half Cauchy prior for regression coefficients
#'@param in_E_inv_tau_sq numeric scalar for the initial value for inverse of tau squared
#'@param in_E_omega numeric vector of length n for the initial values for all omega's
#'@param in_E_beta  numeric vector of length p for the initial values for all beta's
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{inv_tau_sq}{posterior mean of inverse tau square}
#'\item{omega}{a vector of posterior mean of omega}
#'}
#'\item{trace}{a list object of two components for the trace of parameter updates}
#'\describe{
#'\item{inv_tau_sq}{a vector of trace for inverse of tau_sq}
#'\item{sum_beta_sq}{a vector of trace for inverse of summation of beta_sq}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=20,X_cor=0.5,X_var=1,q=10,beta_size=5)
#'split <- train_test_splits(dat1$n)
#'dat1$train_idx <- split$train_idx
#'dat1$test_idx <- split$test_idx
#'res1 <- with(dat1,fast_mfvb_normal_logit_single(y[train_idx],X[train_idx,]))
#'res1_mcmc <- with(dat1,fast_normal_logit(y[train_idx],X[train_idx,]))
#'res1_glmnet <- with(dat1,wrap_glmnet(y[train_idx],X[train_idx,],alpha=0.5,family=binomial()))
#'
#'
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_mcmc$post_mean$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,
#'res1_mcmc$elapsed))
#'
#'rownames(tab)<-c("n = 2000, p = 20 MFVB","n = 2000, p = 20 glmnet",
#'"n = 2000, p = 20 MCMC")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
fast_mfvb_normal_logit_single <- function(y, X, max_iter = 5000L, tol = 1e-05, A = 10, in_E_inv_tau_sq = 1, in_E_omega = NULL, in_E_beta = NULL) {
    .Call(`_fastBayesReg_fast_mfvb_normal_logit_single`, y, X, max_iter, tol, A, in_E_inv_tau_sq, in_E_omega, in_E_beta)
}

#'@title Fast mean field varational Bayesian multinomial logistic regression with normal priors
#'@param y vector of n multiclass outcome variables taking values 0,...,M-1
#'@param X n x p matrix of candidate predictors
#'@param num_class an integer indicating the number of classes
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,beta_size=1,intercept0=c(5,-5,-10,-10))
#'glmnet_res <- with(dat,wrap_glmnet(y,cbind(1,X),family="multinomial"))
#'Bayes_res <- with(dat,fast_mfvb_multiclass(y,cbind(1,X),num_class=length(unique(y)),burnin=5000))
#'glmnet_pred <- as.numeric(predict(glmnet_res$glmnet_fit,newx = cbind(1,dat$X),type = "class"))
#'Bayes_pred <- apply(Bayes_res$post_mean$prob,1,which.max)-1
#'print(c(glmnet_acc = mean(glmnet_pred==dat$y),Bayes_acc = mean(Bayes_pred==dat$y)))
#'@export
fast_mfvb_multiclass <- function(y, X, num_class, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1) {
    .Call(`_fastBayesReg_fast_mfvb_multiclass`, y, X, num_class, mcmc_sample, burnin, thinning, A_tau)
}

#'@title Fast Bayesian logistic regression with horseshoe priors
#'@param y vector of n binary outcome variables taking values 0 or 1
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of three components
#'\describe{
#'\item{post_mean}{a list object of five components for posterior mean statistics}
#'\describe{
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{tau2}{posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{lambda}{a vector of posterior mean of p local shrinkage parameters}
#'\item{mu}{a vector of posterior predictive mean for linear predictor of the n training sample}
#'\item{prob}{a vector of posterior predictive probability of the n training sample}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples for p regression coeficients. Each column is one MCMC sample}
#'\item{tau2}{a vector of MCMC samples of global shrinkage parameters}
#'\item{lambda}{a matrix of MCMC samples of p local shrinkage parameters. Each column is one MCMC sample}
#'}
#'\item{elapsed}{running time}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_logit_reg(n=2000,p=200,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res1 <- with(dat1,fast_horseshoe_logit(y,X,A_tau=0.001,A_lambda=0.001))
#'res1_glmnet <- with(dat1,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'dat2 <- sim_logit_reg(n=200,p=2000,X_cor=0.9,X_var=10,q=10,beta_size=5)
#'res2 <- with(dat2,fast_horseshoe_logit(y,X,burnin=5000,mcmc_sample=500,A_tau=0.001,A_lambda=0.001))
#'res2_glmnet <- with(dat2,wrap_glmnet(y,X,alpha=0.5,family=binomial()))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat1$betacoef,res1_glmnet$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2_glmnet$betacoef)),
#'time=c(res1$elapsed,res1_glmnet$elapsed,res2$elapsed,res2_glmnet$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200 Bayes","n = 2000, p = 200 glmnet",
#'"n = 200, p = 2000 Bayes","n = 200, p = 2000 glmnet")
#'normal_logit_tab <- tab
#'print(normal_logit_tab)
#'@export
fast_horseshoe_logit <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, A_tau = 1, A_lambda = 1) {
    .Call(`_fastBayesReg_fast_horseshoe_logit`, y, X, mcmc_sample, burnin, thinning, A_tau, A_lambda)
}

#'@title Simulate left standard truncated normal distribution
#'@param n sample size
#'@param lower the lower bound
#'@return a vector of random numbers from a left standard truncated
#'normal distribution
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'r0 <- rand_left_trucnorm0(1000, lower=2)
#'hist(r0)
#'@export
rand_left_trucnorm0 <- function(n, lower, ratio = 1.0) {
    .Call(`_fastBayesReg_rand_left_trucnorm0`, n, lower, ratio)
}

#'@title Simulate left truncated normal distribution
#'@param n sample size
#'@param mu the location parameter
#'@param sigma the scale parameter
#'@param lower the lower bound
#'@return a vector of random numbers from a left truncated
#'normal distribution
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'r <- rand_left_trucnorm(1000, mu=100, sigma=0.1, lower=200)
#'hist(r)
#'@export
rand_left_trucnorm <- function(n, mu, sigma, lower, ratio = 1.0) {
    .Call(`_fastBayesReg_rand_left_trucnorm`, n, mu, sigma, lower, ratio)
}

#'@title Simulate right truncated normal distribution
#'@param n sample size
#'@param mu the location parameter
#'@param sigma the scale parameter
#'@param upper the upper bound
#'@return a vector of random numbers from a right truncated
#'normal distribution
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'r <- rand_right_trucnorm(1000, mu=100, sigma=0.1, upper=90)
#'hist(r)
#'@export
rand_right_trucnorm <- function(n, mu, sigma, upper, ratio = 1.0) {
    .Call(`_fastBayesReg_rand_right_trucnorm`, n, mu, sigma, upper, ratio)
}

#'@title Fast Bayesian linear regression with horseshoe priors
#'@param y vector of n outcome variables
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the global shrinkage parameter
#'@param A_lambda scale parameter in the half Cauchy prior of the local shrinkage parameter
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{mu}{a vector of posterior predictive mean of the n training sample}
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{lambda}{a vector of posterior mean of p local shrinkage parameters}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{b_lambda}{posterior mean of the rate parameter in the prior for local shrinkage parameters}
#'\item{tau2}{posterior mean of the global parameter}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples of p regression coeficients}
#'\item{lambda}{a matrix of MCMC samples of p local shrinkage parameters}
#'\item{sigma2_eps}{a vector of MCMC samples of the noise variance}
#'\item{b_lambda}{a vector of MCMC samples of the rate parameter in the prior for local shrinkage parameters}
#'\item{tau2}{a vector of MCMC samples of the global shrinkage parameter}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_horseshoe_lm(y,X))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_horseshoe_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_horseshoe_tab <- tab
#'print(fast_horseshoe_tab)
#'@export
fast_horseshoe_lm <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, a_sigma = 0.0, b_sigma = 0.0, A_tau = 1, A_lambda = 1) {
    .Call(`_fastBayesReg_fast_horseshoe_lm`, y, X, mcmc_sample, burnin, thinning, a_sigma, b_sigma, A_tau, A_lambda)
}

#'@title Fast Bayesian high-dimensional linear regression with horseshoe priors using slice sampler
#'@param y vector of n outcome variables
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the global shrinkage parameter
#'@param A_lambda scale parameter in the half Cauchy prior of the local shrinkage parameter
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{mu}{a vector of posterior predictive mean of the n training sample}
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{lambda}{a vector of posterior mean of p local shrinkage parameters}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{tau2}{posterior mean of the global parameter}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples of p regression coeficients}
#'\item{lambda}{a matrix of MCMC samples of p local shrinkage parameters}
#'\item{sigma2_eps}{a vector of MCMC samples of the noise variance}
#'\item{tau2}{a vector of MCMC samples of the global shrinkage parameter}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_horseshoe_ss_lm(y,X))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_horseshoe_ss_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_horseshoe_tab <- tab
#'print(fast_horseshoe_tab)
#'@export
fast_horseshoe_ss_lm <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, a_sigma = 0.0, b_sigma = 0.0, A_tau = 1, A_lambda = 1) {
    .Call(`_fastBayesReg_fast_horseshoe_ss_lm`, y, X, mcmc_sample, burnin, thinning, a_sigma, b_sigma, A_tau, A_lambda)
}

#'@title Fast Bayesian high-dimensional linear regression with horseshoe priors
#'@param y vector of n outcome variables
#'@param X n x p matrix of candidate predictors
#'@param mcmc_sample number of MCMC iterations saved
#'@param burnin number of iterations before start to save
#'@param thinning number of iterations to skip between two saved iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the global shrinkage parameter
#'@param A_lambda scale parameter in the half Cauchy prior of the local shrinkage parameter
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\describe{
#'\item{mu}{a vector of posterior predictive mean of the n training sample}
#'\item{betacoef}{a vector of posterior mean of p regression coeficients}
#'\item{lambda}{a vector of posterior mean of p local shrinkage parameters}
#'\item{sigma2_eps}{posterior mean of the noise variance}
#'\item{b_lambda}{posterior mean of the rate parameter in the prior for local shrinkage parameters}
#'\item{tau2}{posterior mean of the global parameter}
#'}
#'\item{mcmc}{a list object of three components for MCMC samples}
#'\describe{
#'\item{betacoef}{a matrix of MCMC samples of p regression coeficients}
#'\item{lambda}{a matrix of MCMC samples of p local shrinkage parameters}
#'\item{sigma2_eps}{a vector of MCMC samples of the noise variance}
#'\item{b_lambda}{a vector of MCMC samples of the rate parameter in the prior for local shrinkage parameters}
#'\item{tau2}{a vector of MCMC samples of the global shrinkage parameter}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_horseshoe_ss_lm(y,X))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_horseshoe_ss_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_horseshoe_tab <- tab
#'print(fast_horseshoe_tab)
#'@export
fast_horseshoe_hd_lm <- function(y, X, mcmc_sample = 500L, burnin = 500L, thinning = 1L, a_sigma = 0.0, b_sigma = 0.0, A_tau = 1, A_lambda = 1) {
    .Call(`_fastBayesReg_fast_horseshoe_hd_lm`, y, X, mcmc_sample, burnin, thinning, a_sigma, b_sigma, A_tau, A_lambda)
}

#'@title Prediction with fast Bayesian linear regression fitting
#'@param model_fit  output list object of fast Bayesian linear regression fitting (see value of \link{fast_horseshoe_lm} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@param alpha posterior predictive credible level \eqn{\alpha \in (0,1)}. The default value is \eqn{0.95}.
#'@return a list object consisting of three components
#'\describe{
#'\item{mean}{a vector of \eqn{n} posterior predictive mean values}
#'\item{ucl}{a vector of \eqn{n}  posterior \eqn{\alpha} level upper credible limits}
#'\item{lcl}{a vector of \eqn{n}  posterior \eqn{\alpha} level lower credible limits}
#'\item{median}{a vector of \eqn{n}  posterior predictive median values}
#'\item{sd}{a vector of \eqn{n}  posterior predictive standard deviation values}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'train_idx = 1:round(length(dat$y)/2)
#'test_idx = setdiff(1:length(dat$y),train_idx)
#'res <- fast_horseshoe_lm(dat$y[train_idx],dat$X[train_idx,])
#'pred_res <- predict_fast_lm(res,dat$X[test_idx,])
#'plot(dat$y[test_idx,],pred_res$mean,
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="Observations",
#'ylab = "Predictions")
#'abline(0,1)
#'@export
predict_fast_lm <- function(model_fit, X_test, alpha = 0.95) {
    .Call(`_fastBayesReg_predict_fast_lm`, model_fit, X_test, alpha)
}

#'@title Prediction with fast Bayesian linear regression fitting with multiple outcomes
#'@param model_fit  output list object of fast Bayesian linear regression fitting (see value of \link{fast_horseshoe_lm} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@param alpha posterior predictive credible level \eqn{\alpha \in (0,1)}. The default value is \eqn{0.95}.
#'@return a list object consisting of three components
#'\describe{
#'\item{mean}{a matrix of \eqn{n} by \eqn{q} posterior predictive mean values}
#'\item{ucl}{a matrix of \eqn{n} by \eqn{q}  posterior \eqn{\alpha} level upper credible limits}
#'\item{lcl}{a vector of \eqn{n} by \eqn{q}  posterior \eqn{\alpha} level lower credible limits}
#'\item{median}{a vector of \eqn{n} by \eqn{q}  posterior predictive median values}
#'\item{sd}{a vector of \eqn{n} by \eqn{q}  posterior predictive standard deviation values}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat <- sim_linear_reg_multi(n=2000,p=200,m=5,X_cor=0.9,q=6)
#'train_idx = 1:round(nrow(dat$y)/2)
#'test_idx = setdiff(1:nrow(dat$y),train_idx)
#'res <- fast_normal_multi_lm(dat$y[train_idx,],dat$X[train_idx,])
#'pred_res <- predict_fast_multi_lm(res,dat$X[test_idx,])
#'plot(dat$y[test_idx,1],pred_res$mean[,1],
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="Observations",
#'ylab = "Predictions")
#'abline(0,1)
#'@export
predict_fast_multi_lm <- function(model_fit, X_test) {
    .Call(`_fastBayesReg_predict_fast_multi_lm`, model_fit, X_test)
}

#'@title Prediction with fast mean field variational Bayesian linear regression fitting
#'@param model_fit  output list object of fast Bayesian linear regression fitting (see value of \link{fast_horseshoe_lm} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@param alpha posterior predictive credible level \eqn{\alpha \in (0,1)}. The default value is \eqn{0.95}.
#'@return a list object consisting
#'\describe{
#'\item{mean}{a vector of \eqn{n} posterior predictive mean values}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'train_idx = 1:round(length(dat$y)/2)
#'test_idx = setdiff(1:length(dat$y),train_idx)
#'res <- fast_mfvb_normal_lm(dat$y[train_idx],dat$X[train_idx,])
#'pred_res <- predict_fast_mfvb_lm(res,dat$X[test_idx,])
#'plot(dat$y[test_idx,],pred_res$mean,
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="Observations",
#'ylab = "Predictions")
#'abline(0,1)
#'@export
predict_fast_mfvb_lm <- function(model_fit, X_test) {
    .Call(`_fastBayesReg_predict_fast_mfvb_lm`, model_fit, X_test)
}

#'@title Prediction with fast Bayesian logistic regression fitting
#'@param model_fit  output list object of fast Bayesian logistic regression fitting (see value of \link{fast_horseshoe_lm} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@param alpha posterior predictive credible level \eqn{\alpha \in (0,1)}. The default value is \eqn{0.95}.
#'@param cutoff threshold value for posterior predicitve probablity. The default value is 0.5
#'@return a list object consisting of three components
#'\describe{
#'\item{class}{a vector of \eqn{n} predicted class indicators}
#'\item{mean}{a vector of \eqn{n} posterior predictive mean probabilities}
#'\item{ucl}{a vector of \eqn{n}  posterior \eqn{\alpha} level upper credible limits of probabilities}
#'\item{lcl}{a vector of \eqn{n}  posterior \eqn{\alpha} level lower credible limits of probabilities}
#'\item{median}{a vector of \eqn{n}  posterior predictive median probabilities}
#'\item{sd}{a vector of \eqn{n}  standard deviations of posterior predictive probabilities}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat <- sim_logit_reg(n=2000,p=200,X_cor=0.9,q=6)
#'train_idx = 1:round(length(dat$y)/2)
#'test_idx = setdiff(1:length(dat$y),train_idx)
#'res <- fast_normal_logit(dat$y[train_idx],dat$X[train_idx,])
#'pred_res <- predict_fast_logit(res,dat$X[test_idx,])
#'plot(dat$prob[test_idx,],pred_res$mean,
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="True Probabilities",
#'ylab = "Predicted Probabilities")
#'abline(0,1)
#'print(comp_class_acc(pred_res$class,dat$y[test_idx]))
#'@export
predict_fast_logit <- function(model_fit, X_test, alpha = 0.95, cutoff = 0.5) {
    .Call(`_fastBayesReg_predict_fast_logit`, model_fit, X_test, alpha, cutoff)
}

#'@title Prediction with fast Bayesian multinomial logistic regression fitting
#'@param model_fit  output list object of fast Bayesian multinomial logistic regression fitting (see value of \link{fast_horseshoe_lm} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@return a list object consisting of three components
#'\describe{
#'\item{class}{a vector of \eqn{n} predicted class indicators}
#'\item{mean}{a matrix of \eqn{n} by \eqn{K} posterior predictive mean probabilities}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat<-sim_multiclass_reg(K=5,n=1000,p=20,X_var = 10,X_cor=0.5,q=10,
#'beta_size=1,intercept0=c(5,-5,-10,-10))
#'train_idx = 1:round(length(dat$y)/2)
#'test_idx = setdiff(1:length(dat$y),train_idx)
#'res <- fast_normal_multiclass(dat$y[train_idx],dat$X[train_idx,])
#'pred_res <- predict_fast_multiclass(res,dat$X[test_idx,])
#'mean(pred_res$class!=dat$y[test_idx])
#'@export
predict_fast_multiclass <- function(model_fit, X_test) {
    .Call(`_fastBayesReg_predict_fast_multiclass`, model_fit, X_test)
}

#'@title Prediction with fast mean field variational Bayesian logistic regression fitting
#'@param model_fit output list object of fast mean field variational Bayesian logistic regression fitting (see value of \link{fast_mfvb_normal_logit} as an example)
#'@param X_test \eqn{n} by \eqn{p} matrix of predictors for the test data
#'@param alpha posterior predictive credible level \eqn{\alpha \in (0,1)}. The default value is \eqn{0.95}.
#'@param cutoff threshold value for posterior predicitve probablity. The default value is 0.5
#'@return a list object consisting of three components
#'\describe{
#'\item{class}{a vector of \eqn{n} predicted class indicators}
#'\item{prob}{a vector of \eqn{n} posterior predictive mean probabilities}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'dat <- sim_logit_reg(n=2000,p=200,X_cor=0.9,q=6,beta_size=5)
#'train_idx = 1:round(length(dat$y)/2)
#'test_idx = setdiff(1:length(dat$y),train_idx)
#'res_mcmc <- fast_normal_logit(dat$y[train_idx],dat$X[train_idx,])
#'res <- fast_mfvb_normal_logit(dat$y[train_idx],dat$X[train_idx,])
#'pred_res <- predict_fast_mfvb_logit(res,dat$X[test_idx,])
#'pred_res_mcmc <- predict_fast_logit(res_mcmc,dat$X[test_idx,])
#'par(mfrow=c(1,2))
#'plot(dat$prob[test_idx,],pred_res$prob,
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="True Probabilities",
#'ylab = "Predicted Probabilities",main="MFVB")
#'abline(0,1)
#'plot(dat$prob[test_idx,],pred_res_mcmc$mean,
#'type="p",pch=19,cex=0.5,col="blue",asp=1,xlab="True Probabilities",
#'ylab = "Predicted Probabilities",main="MCMC")
#'abline(0,1)
#'mfvb <- comp_class_acc(pred_res$class,dat$y[test_idx])
#'mcmc <- comp_class_acc(pred_res_mcmc$class,dat$y[test_idx])
#'tab <- rbind(mfvb,mcmc)
#'print(tab)
#'@export
predict_fast_mfvb_logit <- function(model_fit, X_test, alpha = 0.95, cutoff = 0.5) {
    .Call(`_fastBayesReg_predict_fast_mfvb_logit`, model_fit, X_test, alpha, cutoff)
}

#'@title Fast Mean Field Varational Bayesian linear regression with normal priors
#'@param y vector of n outcome variables
#'@param X n x p matrix of candidate predictors
#'@param max_iter max number of iterations
#'@param a_sigma shape parameter in the inverse gamma prior of the noise variance
#'@param b_sigma rate parameter in the inverse gamma prior of the noise variance
#'@param A_tau scale parameter in the half Cauchy prior of the ratio between the coefficient variance and the noise variance
#'@return a list object consisting of two components
#'\describe{
#'\item{post_mean}{a list object of four components for posterior mean statistics}
#'\itemize{
#'\item{mu: a vector of posterior predictive mean of the n training sample}
#'\item{betacoef: a vector of posterior mean of p regression coeficients}
#'\item{sigma2_eps: posterior mean of the noise variance}
#'\item{tau2: posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'}
#'\item{trace}{a list object for parameter updates}
#'\itemize{
#'\item{t_E2: posterior mean of residual squared}
#'\item{t_B2: posterior mean of L2 norm of the regression coeficients}
#'\item{t_tau2: posterior mean of the ratio between prior regression coefficient variances and the noise variance}
#'\item{t_sigma2_eps: posterior mean of the noise variance}
#'}
#'}
#'@author Jian Kang <jiankang@umich.edu>
#'@examples
#'set.seed(2022)
#'dat1 <- sim_linear_reg(n=2000,p=200,X_cor=0.9,q=6)
#'res1 <- with(dat1,fast_mfvb_normal_lm(y,X))
#'dat2 <- sim_linear_reg(n=200,p=2000,X_cor=0.9,q=6)
#'res2 <- with(dat2,fast_mfvb_normal_lm(y,X))
#'tab <- data.frame(rbind(comp_sparse_SSE(dat1$betacoef,res1$post_mean$betacoef),
#'comp_sparse_SSE(dat2$betacoef,res2$post_mean$betacoef)),
#'time=c(res1$elapsed,res2$elapsed))
#'rownames(tab)<-c("n = 2000, p = 200","n = 200, p = 2000")
#'fast_normal_tab <- tab
#'print(fast_normal_tab)
#'@export
fast_mfvb_normal_lm <- function(y, X, max_iter = 500L, a_sigma = 0.01, b_sigma = 0.01, A_tau = 1, tol = 1e-5, t_sigma2_eps_0 = 0, t_tau2_0 = 0) {
    .Call(`_fastBayesReg_fast_mfvb_normal_lm`, y, X, max_iter, a_sigma, b_sigma, A_tau, tol, t_sigma2_eps_0, t_tau2_0)
}

# Register entry points for exported C++ functions
methods::setLoadAction(function(ns) {
    .Call(`_fastBayesReg_RcppExport_registerCCallable`)
})
